{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Books\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTING & CLEANING META DATA\n",
    "\n",
    "meta_products = spark.read.json(DATA_DIR+\"meta_Books.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_corrupt_record',\n",
       " 'asin',\n",
       " 'brand',\n",
       " 'categories',\n",
       " 'description',\n",
       " 'imUrl',\n",
       " 'price',\n",
       " 'related',\n",
       " 'salesRank',\n",
       " 'title']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_products.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will extract only the features and turn them into more readable features.\n",
    "# Features removed : corruptRecord, imURL, related\n",
    "# This will extract only the features and turn them into more readable features.\n",
    "# Filter salesRank = None because this will lead to problems for the writing in parquet\n",
    "# Features removed : corruptRecord, imURL, related\n",
    "#data_cleaned = meta_products.rdd.flatMap(lambda r: [(r[1], r[2], 'Books', r[4], r[6],  r[9] )])\n",
    "data_cleaned = meta_products.rdd.flatMap(lambda r: [(r[1], r[4], r[6], r[9] )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StructType to define the DataFrame that we'll create with the previously extracted rdd table\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    #StructField(\"brand\", StringType(), True),\n",
    "    #StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    #StructField(\"salesRank\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able to store it in Parquet)\n",
    "datacleaned_DF = spark.createDataFrame(data_cleaned, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into parquet to save time in the next times\n",
    "datacleaned_DF.write.mode('overwrite').parquet(\"nobrand_meta_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the parquet data\n",
    "datacleaned_DF = spark.read.parquet(\"meta_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin', 'description', 'price', 'title']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datacleaned_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\" global warming\", \" solar energy\", \" recycling \", \" pollution \", \"solar power\", \" endangered species\", \"air pollution\", \\\n",
    "\" water pollution\", \" wind energy\", \" climate change\", \" wind power\", \" recycle \", \" deforestation\", \" greenhouse effect\", \"environment\", \\\n",
    "\" sustainability \", \" natural resources\", \"alternative energy\", \" climate \", \"global warming\", \"renewable energy\", \" ecology\", \"composting\", \\\n",
    "\" carbon footprint\", \" bio \", \" biosphere \", \" renewable \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with title and description not equal to None\n",
    "# We will then be able to test if those features contains words defined in the keyword vector\n",
    "# The keyword vector represents the thema that we want : ecology, bio etc...\n",
    "filter_products_bio = datacleaned_DF.rdd.filter(lambda r: (r[3] != None) &  (r[1] != None)) \\\n",
    "                    .filter(lambda r: (any(word in r[3].lower() for word in keywords)) | (any(word in r[1].lower() for word in keywords)) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able work and join with review data)\n",
    "DF_filter_products_bio = spark.createDataFrame(filter_products_bio, samplingRatio=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, description: string, price: double, title: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_filter_products_bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTING & CLEANING REVIEWS DATA\n",
    "\n",
    "reviews = spark.read.json(DATA_DIR+\"reviews_Books.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove the review text, reviewer name and summary\n",
    "reviews_cleaned = reviews.rdd.flatMap(lambda r: [(r[0], r[1], r[2], r[4], r[5],r[8])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the StructType to define the DataFrame that we'll create with the previously extracted rdd table\n",
    "schema = StructType([\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"helpful\", ArrayType(IntegerType()), True),\n",
    "    StructField(\"overall\", FloatType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the RDD data into DataFrame (we'll then be able to store it in Parquet)\n",
    "reviews_cleaned_DF = spark.createDataFrame(reviews_cleaned, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save into parquet to save time in the next times\n",
    "reviews_cleaned_DF.write.mode('overwrite').parquet(\"reviews_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_cleaned_DF = spark.read.parquet(\"reviews_Books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[asin: string, helpful: array<int>, overall: float, reviewTime: string, reviewerID: string, unixReviewTime: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_cleaned_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join Reviews and Metadata\n",
    "review_product_join = DF_filter_products_bio.join(reviews_cleaned_DF, ['asin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asin',\n",
       " 'description',\n",
       " 'price',\n",
       " 'title',\n",
       " 'helpful',\n",
       " 'overall',\n",
       " 'reviewTime',\n",
       " 'reviewerID',\n",
       " 'unixReviewTime']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_product_join.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_product_join = review_product_join.filter(review_product_join.brand.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_product_join = review_product_join.filter(review_product_join.price.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "#review_product_join.select([count(when(col(c).isNull(), c)).alias(c) for c in review_product_join.columns]).show()\n",
    "#review_product_join.select([count(when(isnan(c), c)).alias(c) for c in review_product_join.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "review_product_join.write.mode('overwrite').parquet(\"joined_Books_test.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
